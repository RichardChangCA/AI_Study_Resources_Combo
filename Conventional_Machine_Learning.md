1. Anomaly Detection Study Resources Combo: https://github.com/RichardChangCA/Anomaly_detection_practice

2. Medium, PCA explained and implemented in 2 minutes： https://towardsdatascience.com/pca-explained-and-implemented-in-2-minutes-e024c832bb9c ， PCA implementation without scikit-learn

3. Medium, Here’s how you can accelerate your Data Science on GPU : https://towardsdatascience.com/heres-how-you-can-accelerate-your-data-science-on-gpu-4ecf99db3430 use your Nvidia GPU with cuda, cuML is the cuda version scikit-learn and cuDF is cuda version pandas, these libaries can accelerate your data pre-processing or conventional ML models' speed.

4. Medium, Anomaly Detection with MIDAS: https://medium.com/towards-artificial-intelligence/anomaly-detection-with-midas-2735a2e6dce8 ,time-series anomaly detection, this article is not clear but shows an introduction, more details in GitHub: https://github.com/bhatiasiddharth/MIDAS & the paper(AAAI 2020): https://www.comp.nus.edu.sg/~sbhatia/assets/pdf/midas.pdf

5. Medium, Feature Handling in Machine Learning, https://medium.com/analytics-vidhya/feature-handling-in-machine-learning-de5525794b9c ,feature representation: represent as numeric data, feature selection to drop useless features, feature transformation: transform object format data to your needed data format, feature statistics.

6. Medium, Rediscovering Semi-Supervised Learning(SSL), https://medium.com/data-from-the-trenches/re-discovering-semi-supervised-learning-a18bb46116e3 ,use unsupervised method to cluster unlabeled data with labeled data and give unlabeled data with pseudo-label with certain proportion ratio(pseudo-label=ratio*true-label) or uncertainty-based(pseudo-label>threshold denoted as true label, this method is better). give low weight(according to confidence value) on pseudo-label data and more more weight on true-labeled data(no significant difference). And then use supervised learning method with true-labeled data augmentation.

7. Medium, Improve Your Model with Missing Data | Imputation with NumPyro, https://towardsdatascience.com/improve-your-model-with-missing-data-imputation-with-numpyro-dcb3c3376eff ,source code: https://github.com/RMichae1/PyroStudies/blob/master/Bayesian_Imputation.ipynb ,Bayesian Inference Regression.

8. Medium, The Best Machine Learning Resources, https://medium.com/machine-learning-for-humans/how-to-learn-machine-learning-24d53bb64aa1 ,Programming(Python), Linear Algebra, Probability&Statistics, Calculus, Machine Learning, Deep Learning, Reinforcement Learning, Artificial Intelligence, Artificial Intelligence Safety, News Letters, Advices

9. Medium, A Visual Guide to Gradient Boosted Trees (XGBoost), https://towardsdatascience.com/a-visual-guide-to-gradient-boosted-trees-8d9ed578b33 ,Boosting combines weak learners(usually decision trees with only one split, called decision stumps) sequentially, so that each new tree corrects the errors of the previous one. 

10. Optuna, an automatic hyperparameter optimization software framework, particularly designed for machine learning. https://github.com/optuna/optuna

11. Medium, Train multiple Time Series Forecasting Models in one line of Python Code, https://towardsdatascience.com/train-multiple-time-series-forecasting-models-in-one-line-of-python-code-615f2253b67a ,Auto-TS

12. Medium, Time Series Forecasting with PyCaret Regression Module, https://towardsdatascience.com/time-series-forecasting-with-pycaret-regression-module-237b703a0c63 , Conventional Machine Learning algorithms demo

13. Medium, Curse of Batch Normalization, https://towardsdatascience.com/curse-of-batch-normalization-8e6dd20bc304 ,Batch Normalizatino is a widely adopted technique that enables faster and more stable training and has become one of the most influential methods. Benefits: Faster convergence, Decreases the importance of initial weights, Robust to hyperparameters, Requires less data for generalization. Cursed Batch Normalization: Unstable when using small batch sizes(the batch normalization layers has to calculate mean and variance to normalize the previous outputs across the batch, this statistical estimation will be pretty accurate if the batch size is fairly large while keeps on decreasing as the batch size decreases). Batch size cannot be 1 for batch normalization because it will be mean of itself. The per-iteration time could be noticeably increased. After putting these models into production, these models don’t work as good as they were while training. This is because they are trained with large batch size, while in real-time they are getting a batch size equal to one because it has to process each frame subsequently. Considering this limitation, some implementations tends to use pre-computed mean and variances based on the activations on the training set. Another potential is to compute the mean and variation values based on your test-set activation distribution, but still not batch-wise. Not good for online learning. Not good for recurrent neural networks. Alternatives(Layer Normalization, Instance Normalization, Group Normalization, Synchronous Batch Normalization).
