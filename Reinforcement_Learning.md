1. Youtube: 强化学习，周莫烦， https://www.youtube.com/watch?v=NVWBs7b3oGk&list=PLXO45tsB95cJYKCSATwh1M4n8cUnUv6lT 

2. Youtube: 遗传算法，周莫烦，https://www.youtube.com/watch?v=BEquIwfEXes&list=PLXO45tsB95cLleNFWdeIxepBjuyLlBFhf Genetic Algorithm, Evolution Strategy, Neuro-Evolution

3. Medium, A Comprehensive Guide to Genetic Algorithms (and how to code them), https://medium.com/sigmoid/https-medium-com-rishabh-anand-on-the-origin-of-genetic-algorithms-fc927d2e11e0 ,GA for neural network hyper-parameters selection. Model Fitness, Model Selection, Model Crossover, Model Mutation.

4. Medium, A Brief Introduction to Markov Chains, https://medium.com/sigmoid/rl-markov-chains-dbf2f37e8b69 , A Markov Chain is a stochastic(always changing) model that is used to estimate the outcome of an event given only the previous state and its action. Markov Reward Process where the agent(whose objective is to maximize the rewards earned in the task) in the current task is rewarded based on its performance in the current time step.

5. Blog, Reinforcement Q-Learning from Scratch in Python with OpenAI Gym, https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/

6. Youtube, Introduction to Reinforcement Learning by David Silver. https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ

Class 1: Introduction of Reinforcement learning:

Notes: Reinforcement Learning is no supervisor, only a reward signal. Feedback is delayed, not instantaneous. Time really matters(Sequential, non i.i.d. data). Reward Hypothesis: All goals can be described by the maximization of expected cumulative reward. Sequential Decision Making: (Goal) select actions to maximize total future reward, it may be better to sacrifice immediate reward to gain more long-term reward. State is the information used to determine what happens next and state is a function of the history. The environment state S_t^e is the environment’s private representation, the environment state is not usually visible to the agent and even if S_t^e is visible, it may contain irrelevant information. The agent state S_t^a is the agent’s internal representation, and it is the information used by reinforcement learning algorithms. An information state(a.k.a Markov state) contains all useful information form the history. Once the state is known, the history may be thrown away, the state is a sufficient statistic of the future. Full observability: agent directly observes environment state: O_t=S_t^a=S_t^e, agent state=environment state=information state, formally, this is a Markov Decision Process(MDP). Partial observability: agent indirectly observes environment, here now agent state is unequal with environment state, formally this is a partially observable Markov Decision Process(POMDP), agent must construct its own state representation S_t^a(e.g. Beliefs of environment state, recurrent neural network). Categorizing RL agents(Value based: Value Function, Policy based: Policy, Actor Critic: Policy + Value Function, Model free: Policy and/or Value Function and no model, Model based: Policy and/or Value Function and with model). Reinforcement learning is like trial-and-error learning. Exploration finds more information about the environment. Exploitation exploits known information to maximize reward. (Trade-off Exploration and Exploitation).
