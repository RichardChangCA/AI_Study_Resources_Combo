\* Codes, Image Captioning: https://github.com/RichardChangCA/CSI_5386_NLP_Project

1. Youtube, Automatic image captioning: https://www.youtube.com/watch?v=yk6XDFm3J2c ,this presentation combines CNN and RNN(especially LSTM), given image inputs, generating natural languages as outputs. Although this presentation was published in 2015, the idea behind image captioning is not outdated.

2. Medium, Illustrated self-attention: https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a ,the basic idea behind Transformer, Bert and etc. in NLP. This idea replaced the RNN family to extract whole sentence words together.

3. Medium, Illustrated Attention: https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3 , a good article, including attention mechanism, score function

- seq2seq

- seq2seq + attention

- seq2seq with bidirectional encoder + attention

- seq2seq with 2-stacked encoder + attention

- GNMT(Google Neural Machine Translation) — seq2seq with 8-stacked encoder (+bidirection+residual connections) + attention

4. Medium, Image Captioning with Keras, https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8 ,How to Prepare a Photo Caption Dataset for Training a Deep Learning Model, https://machinelearningmastery.com/prepare-photo-caption-dataset-training-deep-learning-model/ , cnn+rnn+transfer learning

5. Medium, The Mechanics of Attention Mechanism in Flowcharts: https://towardsdatascience.com/the-mechanics-of-attention-mechanism-f6e9805cca66 ,the attention mechanism in machine translation, attention mechanism can obtain the context of whole original text information to help translate into another language. two version attention: soft(softmax function, focus on a large scope with various weights), hard(max function, only focus on the most important word), soft attention performs better than hard attention in most cases probably because soft attention can be differentiable. the attention for each pair of words can be visualized.

6. Youtube, attention is all you need, https://www.youtube.com/watch?v=iDulhoQ2pro&t=941s ,the self-attention mechanism in natural language processing, especially in SOTA models, like bert, transformer, xlnet. the idea is also used in computer vision, like self-attention GAN, self-attention unet etc. There are three basic concepts in self-attention: key, value, and query(K,V,Q), key and value pairs are generated from input sentences(original sentences in machine translation), and query is generated from the previous words before the next predicting word in output sentences. After that, key and query do a dot-product to calculate the similarity between inputs and outputs(a.k.a finding the attention between inputs and outputs), and then use softmax to choose the best value for the next word prediction. The most important difference between attention and self-attention is that: (1. attention needs more parameters to store the attention values(a.k.a others supervise you) (2. self-attention can only use inputs and part of outputs theirselves to know where they should pay "closer" attention, no more parameters needed(a.k.a you has self-displine)

7. 《深入浅出词嵌入技术》 Static word embedding(pre-trained word embedding will be used everywhere without any changes), contextualized(dynamic) word embedding, one-hot encoding cannot represent word semantic and is very sparse, CBOW(predict a word from the context), skip-gram(predict the context from the headword, window size, because of the high computational complexity O(|V|), negative sampling & hierarchical softmax, local method), small data and infrequent word—> skip-gram is better. Co-occurrence matrix, matrix factorization(SVD because of the sparsity, global method), glove(global vectors for word representation, combine global and local methods), Gaussian embedding(word—>distribution, similarity—>KL Divergence to evaluate the distance between two distributions), pointcare embedding(some data has inherent hierarchical structure, non-euclidean space), neural network language model(predict a word according to previous N words). Contextualized embedding(tackle with polysemy), CoVe(Contextualized Word Vectors, seq2seq(e.g. RNNs), encoder can extract the context, combine context vector and pre-trained word embedding(e.g. glove)). ELMo(Deep Contextualized Word Representations, autoregressive model, only use encoder without decoder). Sequential models(cannot process parallelly) have the problem of gradient exploding(although use attention mechanism) and are hard to capture long-term dependency. Transformer models(self-attention) eliminate the problem of long-term dependency because each pair of words in inputs has their presentations. Self-attention layers(Q,K,V matrix to calculate the relation between contexts). GPT-2(Transformer-Decoder, the problem is uni-directional during prediction(tackled by Bert and XLNet)). Bert(mask token, mask some words randomly and predict these words during training, BERT’s objective is originally inherited from DAE(denoising autoencoder, X+delta—>f—>X), combine transformer encoder and DAE, Problem:Training and Testing discrepancy(mask in training not in testing), independent assumption of predicted tokens). MASS(Masked Sequence to Sequence Pre-training for Language Generation, remove the paragraph, and predict using decoder. More suitable for generation tasks e.g. machine translation). RoBERTa(A Robustly Optimized BERT Pretraining Approach, Dynamically changing the masking pattern applied to the training data). Autoregressive(sequential language model, problem: only consider previous words, benefits: suitable for generative sentence) vs Autoencoding(bert, problem:discrepancy and independent assumption and not suitable for generative sentence, benefits: consider the context). XLNet(Autoregressive Pre-training Model, combine benefits of Autoregressive and Autoencoding, key idea: consider all possible factorizations(permutation), two-stream self-attention). Model Compression(Sparse Priors(i.e. Beyesian Compression, Bayesian Compression for Deep Learning), Sparse Matrix Factorization(i.e. huge sparse parameter matrix, ALBERT), knowledge distillation(student-teacher model, TinyBert)).

8. Youtube, Course 5, deeplearning.ai Andrew Ng, Sequence Models, https://www.youtube.com/watch?v=_i3aqgKVNQI&list=PLkDaE6sCZn6F6wUI9tvS_Gw1vaFAx6rd6&index=1 ,sequence to sequence, image captioning, machine translation, conditional language model(probability maximization), BLEU score(n-grams, brevity penalty), attention model, speech recognition(connectionist temporal classification), complete courses are in Coursera: https://www.coursera.org/learn/nlp-sequence-models#syllabus NLP courses

9. Coursera, Andrew Ng, deeplearning.ai ,Sequence Models, https://www.coursera.org/learn/nlp-sequence-models#syllabus ,vanilla RNN, bidirectional RNN, back propagation through time, many-to-many RNN architectures, sentiment classification(many-to-one RNN), music generation(one-to-many RNN, the previous prediction as the input of the current RNN unit),  machine translation(encoder-decoder, seq2seq), vanishing gradients, exploding gradients(gradient clipping), Gated Recurrent Unit(GRU), LSTM(Long Short-Term Memory, forget gate, update gate, output gate), Bidirectional RNN, deep stacked RNN, named entity recognition example, word embedding, cosine similarity, embedding matrix, Word2Vec skip-gram(hierarchical softmax —> unbalanced tree structure, context(with probability to sample common and uncommon words), target, computationally expensive in softmax function due to the consideration of all words, #words binary classification problem),  CBOW(predict the middle word according to the context), negative sampling(context, context word/random word, target(positive/negative examples), (random words amount k)k+1 binary classification problem, computation cost is much lower than skip-gram), Glove(X_ct: #times t appears in context of c, X_ct=X_tc, weighted function: large weight for frequent words & vice versa), sentiment classification example, debasing word embeddings(ethical bias,1.Identify bias direction 2.Neutralize: For every word that is not definitional, project to get rid of bias 3.Equalize pairs), finding the most likely translation—>beam search(beam width hyper-parameter, bream width=1–>greedy search, soft length normalization with log, beam search runs faster but is not guaranteed to find exact maxima, error analysis—>analyze which parts of the project should be blamed(beam search or RNN model?))

10. Medium, Top 10 Books on NLP and Text Analysis, https://medium.com/sciforce/top-10-books-on-nlp-and-text-analysis-8393a9fd3f49 ,Python, NLTK library, statistical NLP, linguistics, R programming language, text mining by tidytext package and others(R),  neural network models, Apache-based NLP(Java for enterprise-grade NLP projects), deep learning, text analysis with python, scikit-learn and TensorFlow. 
