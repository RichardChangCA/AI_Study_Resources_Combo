1. Youtube, Graph Convolutional Neural Networks, https://www.youtube.com/watch?v=Mf-mIRF3ao8 ,AI learns(GCN) structure(Graph) of data points. Irregular data structures. Adjacency Matrix in graph. Node classification: use label nodes to predict unable nodes. Graph classification: predict information about new graphs, based on labeled graphs. CNN: spatial invariance properties. Graph-Shifted Signal: idea from the DSP field, linear shift invariant filter can be represented as a polynomial of shifts.Youtube Linear Filters: https://www.youtube.com/watch?v=WeNpd_YEF6I ,blur filters, sharper filter. ECE course: Linearity, Shift Invariance: https://www.youtube.com/watch?v=xhDGX8nHAdg

GNN Tools: PyTorch Geometric, Graph Nets(TensorFlow), Deep Graph(easy for beginners)

2. Youtube, Graph Network, Siraj Raval. https://www.youtube.com/watch?v=bA261BF0bdk ,Simple intuitive introduction of GNN, Sample neighbourhood —> Aggregate feature information from neighbours —> Predict graph context and label using aggregated information.

3. CS224W: Machine Learning with Graphs Stanford/Fall 2019

Official Website: http://web.stanford.edu/class/cs224w/index.html

Video: http://snap.stanford.edu/class/cs224w-videos-2019/

Youtube Channel: https://www.youtube.com/watch?v=dD6LRgw_2mQ&list=PL1OaWjIc3zJ4xhom40qFY5jkZfyO5EDOZ&index=1

Bilibili: https://www.bilibili.com/video/BV1jE41177A4?p=2

<b>Lecture 1: Introduction Structure of Graphs</b>

Node Classification: Predict the type/colour of the given node; Link prediction: Predict whether two nodes are linked; Community detection: Identify densely linked clusters of nodes; Network similarity: Measure similarity of two nodes/networks. Bipartite graph is a graph whose nodes can be divided into two disjoint sets U and V such that every link connects a node in U to one in V; that is, U and V are independent sets. Most real-world networks are sparse. Possible options for edge attributes: weights, ranking, type, sign(positive or negative). Bridge edge: if we erase the edge, the graph becomes disconnected. Articulation node: if we erase the node, the graph becomes disconnected. Strongly connected directed graph has a path from each node to every other node and vice versa. Weakly connected directed graph is connected if we disregard the edge directions. Strongly connected components(SCCs) can be identified, but not every node is part of non-trival strongly connected component, in-component: nodes that can reach the SCC, out-component: nodes that can be reached from the SCC.

<b>Lecture 2: Properties of Networks and Random Graph Models</b>

Degree distribution, path, distance, the consequence in directed graphs: distance is not symmetric. Diameter: the maximum (shortest path) distance between any pair of nodes in a graph. Average path length. Clustering coefficient(for undirected graphs, how connected are i’s neighbours to each other? Average clustering coefficient, very useful for social networks), size of the largest connected component: largest set where any two vertices can be joined by a path(largest component=giant component). Random Graph Model( G_np: undirected graph on n nodes where each edge (u, v) appears i.i.d with probability p, G_nm: undirected graph with n nodes, and m edges picked uniformly at random), expansion alpha. In real world social network: high clustering coefficient and low diameter.(graph model with high clustering coefficient and high diameter introduces randomness(shortcuts with probability), which is the Watts Strogatz Model —> high clustering coefficient and low diameter).

Kronecker graphs: Kronecker product is a way of generating self-similar matrices, define a Kronecker product of two graphs as a Kronecker product of their adjacency matrices. Kronecker Graph Model: Generating large realistic graphs. A recursive model of network structure. Kronecker product. Kronecker graph is obtained by growing sequence of graphs by iterating the Kronecker product over the initiator matrix K_1. Stochastic Kronecker Graphs: probability matrix. Fast Kronecker generator algorithm. 

<b>Lecture 3 Motifs and Structural Roles in Networks:</b>

Motifs:Subnetworks, negative significance: under-representation, positive significance: over-representation, network significance profile, Network motifs:”recurring, significant patterns of interconnections”, Pattern: small induced subgraph, Recurring: Found many times(i.e. with high frequency), Significant: more frequent than expected(i.e. in randomly generated networks), Significance of a Motif: Subgraphs that occur in a real network much more often than in a random network have functional significance. Z score captures statistical significance of motif. Network significance profile(SP) is a vector of normalized Z-scores. Significance profile emphasizes relative significance of subgraphs and generally, larger networks display higher Z-scores. Configuration Model: Generate a random graph with a given degree sequence k_1, k_2, …, k_N. Switching configuration model. Graphlets: connected non-isomorphic subgraphs. Graphlet degree vector(GDV): a vector with the frequency of the node in each orbit position, uses graphlets to obtain a node-level subgraph metric and counts #(graphlets) that a node touches.  Automorphism orbit takes into account the symmetries of a subgraph. Graphlet degree vector provides a measure of a node’s local network topology. Finding size-k motifs and graphlets requires: enumerating all size-k connected subgraphs and counting #(occurrences of each subgraph type). Just knowing if a certain subgraph exists in a graph is a hard computational problem(NP-complete) and computation time grows exponentially as the size of the motif and graphlet increases, so feasible motif size is usually small(2 to 8). ESU(Exact Subgraph Enumeration) ESU tree. Graph Isomorphism: Graphs G and H are isomorphic if there exists a bijection f:V(G)—>V(H){high computational complexity} such that: Any two nodes u and v of G are adjacent in G iff f(u) and f(v) are adjacent in H.

Structural Roles in Networks: Roles(A group of nodes with similar structural properties), Communities/Groups(A group of nodes that are well-connected to each other), Structural equivalence(Nodes u and v are structural equivalent if they have the same relationships to all other nodes). RoIX: Automatic discovery of nodes’ structural roles in networks, Recursive feature extraction(Aggregate features of a node and use them to generate new recursive features). Egonet features: computed on the node’s ego net. Egonet includes the node, its neighbours, and any edges in the induced subgraph on these nodes.

4. Medium, An Illustrated Guide to Graph Neural Networks(GNN) https://medium.com/dair-ai/an-illustrated-guide-to-graph-neural-networks-d5564a551783

5. Medium, Getting the Intuition of Graph Neural Networks, https://medium.com/analytics-vidhya/getting-the-intuition-of-graph-neural-networks-a30a2c34280d ,graph theory. Translate graph into features or neural networks: Adjacency Matrix, Node Attributes Matrix, Edge Attributes Matrix. Batch Mode(each item is a graph), Single Mode.

6. Medium, Understanding Graph Convolutional Networks for Node Classification, https://towardsdatascience.com/understanding-graph-convolutional-networks-for-node-classification-a2bfdb7aba7b ,use Eigen-decomposition to reduce dimensionality and perform clustering. The ‘magic’ of GCN is that it can learn features representation even without training or back-propagation.

7. Medium, Deep learning on graphs: successes, challenges, and next steps, https://towardsdatascience.com/deep-learning-on-graphs-successes-challenges-and-next-steps-7d9ec220ba8 ,node-wise problem: identify malicious users in a social network, graph-wise problem: predict solubility of a molecule. The key to efficient learning on graphs is designing local operations with shared weights that do message passing between every node and its neighbours. A major difference compared to classical deep neural networks dealing with grid-structured data is that on graphs such operations are permutation-invariant, independent of the order of neighbour nodes, as there is usually no canonical way of ordering them.

8. Medium, Graph Convolutional Networks (GCN), https://medium.com/ai-in-plain-english/graph-convolutional-networks-gcn-baf337d5cb6b ,Tasks on Graphs(node classification: predict a type of a given node, link prediction: predict whether two nodes are linked, community detection: identify densely linked clusters of nodes, network similarity: how similar are two sub-networks). In the graph, we have node features(the data of nodes) and the structure of the graph(how nodes are connected). GCN is a type of convolutional neural network that can work directly on graphs and take advantage of their structural information. It solves the problem of classifying nodes in a graph, where labels are only available for a small subset of nodes(semi-supervised learning). The general idea of GCN: for each node, we get the feature information from all its neighbours and of course, the feature of itself, assume we use the average() function, we will do the same for all the nodes, finally, we feed these average values into a neural network. Put more weights on the nodes that have low-degree and reduce the impact of high-degree nodes, the idea of this weighted average is that we assume low-degree nodes would have bigger impacts on their neighbours, whereas high-degree nodes generate lower impacts as they scatter their influence at too many neighbours. When stacking another layer on top of the first one, we repeat the gathering info process, but this time, the neighbours already have information about their own neighbours(from the previous step), it makes the number of layers as the maximum number of hops that each node can travel, so, depends on how far we think a node should get information from the networks, we can config a proper number for #layers, but again, in the graph, normally we donot want to go too far, with 6-7 hops, we almost get the entire graph which makes the aggregation less meaningful. GCNs are used for semi-supervised learning on the graph. GCNs use both node features and the structure for the training. The main idea of the GCN is to take the weighted average of all neighbours’ node features(including itself): low-degree nodes get larger weights, then, we pass the resulting feature vectors through a neural network for training. We can stack more layers to make GCNs deeper, consider residual connections for deep GCNs, normally, we go for 2 or 3-layer GCN. Math node: when seeing a diagonal matrix, think of matrix scaling.

9. Paper 2017, SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS, https://arxiv.org/pdf/1609.02907.pdf ,

10. Blog, Laplacian Regularized Least Squares(semi-supervised learning), https://www.futurelearn.com/courses/advanced-machine-learning/0/steps/49579 ,

11. Medium, Facebook’s PyGraph is an Open Source Framework for Capturing Knowledge in Large Graphs, https://medium.com/dataseries/facebooks-pygraph-is-an-open-source-framework-for-capturing-knowledge-in-large-graphs-b52c0fb902e8 ,graph partitioning(does not fully load into memory), multi-threaded computation, distributed execution.

12. Medium, Drug Discovery with Graph Neural Networks — part 1, https://towardsdatascience.com/drug-discovery-with-graph-neural-networks-part-1-1011713185eb ,Fingerprints is a binary vector where each bit represents whether a certain substructure of the molecule is present or not. It is usually quite long and might fail to incorporate some structural information such as chirality. Predict the solubility of a molecule. (Regression task)

13. Medium, Drug Discovery with Graph Neural Networks — part 2, https://towardsdatascience.com/drug-discovery-with-graph-neural-networks-part-2-b1b8d60180c4 ,determines whether the drug can pass safety tests-toxicity. (Classification task)

14. Medium, Drug Discovery with Graph Neural Networks — part 3, https://towardsdatascience.com/drug-discovery-with-graph-neural-networks-part-3-c0c13e3e3f6d ,GNNExplainer, GNN models interpretability.

15. Medium, Feature Extraction for Graphs, https://towardsdatascience.com/feature-extraction-for-graphs-625f4c5fb8cd ,node level, graph level, and neighbourhood overlap features.

16. Medium, Machine Learning Tasks on Graphs, https://towardsdatascience.com/machine-learning-tasks-on-graphs-7bc8f175119a ,node classification, link prediction, learning over the whole graph, community detection.
