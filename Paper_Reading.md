<h3> Resources </h3>

1. Youtube, Two Minute Papers: https://www.youtube.com/user/keeroyz/videos

2. Website, Distill: https://distill.pub ,online journel with visual explanations

<h3> Articles </h3>

1. Medium, Deep Learning Research and How to Get Immersed: https://towardsdatascience.com/deep-learning-research-and-how-to-get-immersed-8bab98c20577

Consider the structure and context of the paper

Several questions during reading papers:

- What are the biggest unanswered questions in this field?

- When and why is the result useful?

- What were the constraints of the research?

- Are any of the insights transferable to other fields?

- Why do some ideas receive wider reach while others fall flat?

- What are the authors not discussing in the paper?

Read both Abstract and Introduction at first.

Pay closer attention to the methods and results.

<h3> Papers </h3>

1. 2019, Multi-Resolution CNN and Knowledge Transfer for Candidate Classification in Lung Nodule Detection, https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8662660 ,Multi-resolution CNN, Knowledge transfer, the size of lung nodule varies, some tissues look similar with lung nodules.

2. 2016, 3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation, https://arxiv.org/pdf/1606.06650.pdf ,3D Biomedical Volumetric Image Segmentation, Semi-automated segmentation: the user annotates some slices of each volume to be segmented. The network predicts the dense segmentation. more slices, better performance. Fully-automated segmentation: the network is trained with annotated slices from a representative training set and can be run on non-annotated volumes. Semi-automated better than Fully-automated. sparsely annotated training data. rigid transformations and slight elastic deformations still yield biologically plausible images. batch normalization(BN) before each ReLU: each batch is normalized during training with its mean and standard deviation and global statistics are updated using these values. This is followed by a layer to learn scale and bias explicitly. At test time, normalization is done via these computed global statistics and the learned scale and bias. weighted softmax loss function. Setting the weights of unlabeled pixels to zero makes it possible to learn from only the labelled ones and, hence, to generalize to the whole volume. performance gain of 3D to 2D.

3. 2017, Automatic Segmentation and Overall Survival Prediction in Gliomas using Fully Convolutional Neural Network and Texture Analysis, https://arxiv.org/pdf/1712.02066.pdf ,dataset: BraTS 2017. 3D connected component analysis -> discard components below a certain threshold -> reduce false positive. predict the prognosis of subject. FCNN -> extracted features(both low and complex level features) along with age of the subject -> Extreme gradient boosting(XGBOOST) regressor to predict the prognosis of the subject. biomedical images -> similar structure -> small dataset & less epochs enough. weighted cross entropy loss. Pyradiomics: python package -> extract features from medical images.

4. 2019, Random 2.5D U-net for Fully 3D Segmentation, https://arxiv.org/pdf/1910.10398.pdf ,maximum intensity project or the Radon-transform to project 3D volumes into 2D images, learnable reconstruction algorithm to lift 2D projection images to volumetric data. Maximum intensity projection for high sparsity. 

5. 2019, Projection-Based 2.5D U-net Architecture for Fast Volumetric Segmentation, https://arxiv.org/pdf/1902.00347.pdf ,maximum intensity projections from different directions,  the shortage of 3D-Unet: huge memory requirements and long training time, the shortage of 2D-Unet: the loss of connection between the slice images, equidistant angles for projection directions

6. 2016, V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation, https://arxiv.org/pdf/1606.04797.pdf ,data augmentation(random non-linear transformation & histogram matching), reduce resolution by using appropriate stride, residual function in each stage, differentiated Dice with respect to the j-th voxel of the prediction as the loss function to replace to the re-weighted loss, data augmentation: control-points and B-spline interpolation & histogram matching

7. 2019, Ensemble of Fully Convolutional Neural Network for Brain Tumor Segmentation from Magnetic Resonance Images, https://link.springer.com/content/pdf/10.1007%2F978-3-030-11726-9.pdf ,2*3D+2D, post-processing(CRF, class-wise 3-D connected component analysis: all components within each class were retrained while the rest were discarded), ensemble aids in reducing the variance associated in the prediction and also helped in increasing quality of the segmentation generated. 

8. 2019, 3D Dilated Multi-Fiber Network for Real-time Brain Tumor Segmentation in MRI, https://arxiv.org/pdf/1904.03355.pdf ,reduce computation cost, multi-modal MRI scans is a challenging task due to the heterogeneous appearance and shape of gliomas, the multi-fibre unit takes advantage of a multiplexer for information routing, channel grouping(fibres) is to split the convolutional channels as multiple groups that can reduce the connections between the feature maps and kernels for parameter saving significantly, two 1*1*1 convolutions(squeeze and then inflate channels) can reduce half of the parameters as compared to using one 1*1*1 convolution, dilated fibre with various dilated rates—>learnable weights for each dilated branches to select most valuable information automatically from different field of view, lightweight and efficient Dilated Multi-Fiber network(group convolution, learnable weighted 3D dilated convolution to gain multi-scale image representation)

9. 2019, MASK-RCNN AND U-NET ENSEMBLED FOR NUCLEI SEGMENTATION, https://arxiv.org/pdf/1901.10170.pdf ,open source package CellProfiler supports adding neural networks to its processing pipeline, Mask-RCNN: find the bounding boxes(good object detection) and then segmentation, which is worse than U-Net,  segmentation overlap between two models and above the threshold are recognized as final segmentation, watershed post-processing for U-Net, different  kinds of segmentation errors in Mask-RCNN & U-Net and ensemble them can get better performance. 

10. 2018, 3D MRI brain tumor segmentation using autoencoder regularization, https://arxiv.org/pdf/1810.11654.pdf ,Due to a limited training dataset size, a variational auto-encoder branch is added to reconstruct the input image itself in order to regularize the shared decoder and impose additional constraints on its layers. The VAE branch reconstructs the input image into itself, and is used only during training to regularize the shared encoder. Group Normalization shows better than Batch Normalization performance when batch size is small. 3D bilinear upsampling with 1*1*1 convolution for upsampling, to reduce channels. Sigmoid function in multi-channels rather than softmax function in the last layer.  Combined loss: dice loss + 0.1 * L2 distance + 0.1 * KL divergence.

11. 2014, R-CNN, Rich feature hierarchies for accurate object detection and semantic segmentation Tech report (v5) , https://arxiv.org/pdf/1311.2524.pdf , OverFeat: sliding-window detector based on a similar CNN architecture, R-CNN: input image —> extract region proposals —> compute CNN features(extract fixed-length feature vector) —> classify regions(linear SVMs), various category-independent region proposals methods(selective search in R-CNN), semantic segmentation: support vector regresssion(SVR) —> multi-scale per-pixel classifier

12. 2018, AnatomyNet: Deep learning for fast and fully automated whole-volume segmentation of head and neck anatomy, https://aapm.onlinelibrary.wiley.com/doi/pdf/10.1002/mp.13300?casa_token=Ev2mBXkz0joAAAAA:ONW24Vd4DQoKDEhVSBOB_00oQbiUaNE92YR8zYDZI2pT68laqzILXb3VzRc6c7rfpaVx2KVgOLm3H78 ,based-on 3D-UNet, a)a new encoding scheme to allow auto segmentation on whole-volume CT images, b)incorporating 3D squeeze-and-excitation residual blocks in encoding layers, c)a new loss function combining Dice scores and focal loss. Address two main challenges 1)small anatomies occupying only a few slices 2)training with inconsistent data annotations with missing ground truth for some anatomical structures. Traditional atlas-based anatomy segmentation methods is difficult to handle anatomy variations among patients. Missing annotation problem: masked and weighted loss function, too much downsampling may have a negative impact on small anomalies, hybrid loss(dice loss + focal loss, the dice loss learns the class distribution alleviating the imbalanced voxel problem, where as the focal loss forces the model to learn poorly classified voxels better, modified hybrid loss to handle missing annotations problem), the U-Net with one downsampling layer works better on small organ segmentation than the standard U-Net probably because downsampling reduces image resolution and makes it harder to segment small organs.

13. 2017, ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices, http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0642.pdf ,point-wise group convolution(first introduced in AlexNet), channel shuffle, group number g controls the connection sparsity of point wise convolutions

14. 2019, Squeeze-and-Excitation Networks, https://arxiv.org/pdf/1709.01507.pdf ,adaptively recalibrate channel-wise feature responses by explicitly modelling interdependencies between channels, author claim that providing the unit with a mechanism to explicitly model dynamic non-linear dependencies between channels using global information can ease the learning process, and significantly enhance the representational power of the network. Squeeze: Global Information Embedding. Excitation: Adaptive Recalibration, gating mechanism(bottleneck with two fully-connected(FC) layers around the non-linearity in the output of squeeze block, dimensionality-reduction layer with reduction ratio r and then dimensionality-increasing layer returning to the channel dimension of the transformation output U). F_scale refers to channel-wise multiplication between the scalar s and the feature map u. SE-block has more computational burden, trade-off between the number of parameters and the performance(the majority of parameters come from the final stage of the network, so in practice, remove SE-block on final stage with only small cost in performance), SE blocks consistently improve performance across different depths with an extremely small increase in computational complexity, 

15. 2020, Segmentation Loss Odyssey, https://arxiv.org/pdf/2005.13449.pdf ,various loss function in deep learning-based medical image segmentation methods, PyTorch implementations of these loss functions are https://github.com/JunMa11/SegLoss 

16. 2017, Aggregated Residual Transformations for Deep Neural Networks(ResNeXt), https://arxiv.org/pdf/1611.05431.pdf ,a new dimension: cardinality(the size of the set of transformations, in my words, it is the number of single blocks in Inception blocks), even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Increasing cardinality is more effective than going deeper or wider when we increase the capacity. Multi-branch convolutional networks + grouped convolutions + compressing convolutional networks + ensembling(aggregation)

17. 2018, Focal Loss for Dense Object Detection, https://arxiv.org/pdf/1708.02002.pdf ,two-stage object detectors(R-CNN): generates a sparse set of candidate object locations + classifies each candidate location as one of the foreground classes or as background using a convolutional neural network, one-stage detectors(YOLO, SSD). Intuitively, this scaling factor can automatically down-weight the contribution of easy examples during training and rapidly focus the model on hard examples. A simple one-stage object detector: RetinaNet —> dense sampling of object locations in an input image & feature pyramid & anchor boxes, class imbalance and model initialization: a “prior” for the value of p estimated by the model for the rare class(foreground) at the start of training, class imbalance and two-stage detectors: first cascade stage is an object proposal mechanism that reduces the nearly infinite set of possible object locations down to one or two thousand and biased mini batch sampling. RetinaNet: ResNet backbone + Feature Pyramid Network(FPN) + two branch subnets(classifying anchor boxes + regressing bounding boxes). 

18. 2017, Feature Pyramid Networks for Object Detection, https://arxiv.org/pdf/1612.03144.pdf ,Feature pyramids are a basic component in recognition systems for detecting objects at different scales. A top down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. an architecture that combines low-resolution, semantically strong features with high-resolution, semantically weak features via a top-down pathway and lateral connections(merged by addition). A feature pyramid that has rich semantics at all levels and is built quickly from a single input image scale. Append a 3*3 convolution on each merged map to generate the final feature map, which is to reduce the aliasing effect of upsampling. 

19. 2020 CVPR, CascadePSP: Toward Class-Agnostic and Very High-Resolution Segmentation via Global and Local Refinement, https://arxiv.org/pdf/2005.02551.pdf ,a novel approach to address the high-resolution segmentation problem without using any high-resolution training data. A general segmentation refinement framework for refining any input segmentations and achieve a higher accuracy without any fine-tuning afterward.  CascadePSP performs high-resolution(up to 4K) segmentation refinement even our model has never seen any high-resolution training images. With a single refinement module trained on low-resolution data without any fine-tuning, the proposed Global step refines the entire image and provides sufficient image context for the subsequent Local step to perform full-resolution high-quality refinement. CascadePSP is a general segmentation refinement model that refines any given segmentation from low to high resolution. CascadePSP a general cascade segmentation refinement model that can refine any given input segmentations, boosting the performance of state-of-the-art segmentation models without fine-tuning. The Global step considers the whole resized image to repair structure while the Local step refines details in full resolution using image crops. While the Global step is sufficient for low-resolution inputs, the Local step is crucial for accurate high-resolution refinement with size higher than the switching point 900. The lack of a high-resolution image segmentation dataset is one of the difficulties of evaluating an image segmentation model in high-resolution —> BIG dataset is introduced. Scene parsing: divide-and-conquer approach: independently refine each semantic object using our pertained network, followed by integrating the results using a fusion function. Naive approach would be to use argmax on the output confidence which would lead to noisy results in regions where all the classes have low scores. Instead, out fusion function is a modified argmax where if all the input class confidence have values lower than 0.5, we fall back to the original segmentation. 

20. 2019 DARTS: DENSEUNET-BASED AUTOMATIC RAPID TOOL FOR BRAIN SEGMENTATION, https://arxiv.org/pdf/1911.05567.pdf ,GitHub: https://github.com/NYUMedML/DARTS ,weighted loss function: weighing each ROI in inverse proportionality to its average size to address the extreme class imbalance. DenseUNet: dense convolution + UNet. Freesurfer(current tools for brain segmentation) is based on topological surface mappings to detect gray/white matter boundaries followed by nonlinear atlas registration and nonlinear spherical surface registration for each sub-cortical segment —> computationally intensive. Based on our empirical observation, the network trained with only weighted dice loss was unable to escape local optimum and did not converge. Also, empirically it was seen that the stability of the model, in terms of convergence, decreased as the number of classes and class imbalance increased. We found that weighted cross-entropy loss, on the other hand, did not get stuck in any local optima and learned reasonably good segmentations. As the model’s performance with regard to dice score flattened out, we switched from weighted cross entropy to weighted dice loss.Dice loss penalizes both over prediction(especially) and under prediction and, hence, is well-suited for segmentation tasks such as the one proposed here and is particularly useful in medical imaging. Overall dice scores for smaller regions tend to be lower than the dice scores for larger regions.

21. 2020, Rethinking Bottleneck Structure for Efficient Mobile Network Design(MobileNeXt), https://arxiv.org/pdf/2007.02269.pdf ,sandglass block: performs identity mapping and spatial transformation at higher dimensions and thus alleviates information loss and gradient confusion effectively. Identity tensor multiplier controls what portion of the channels in the identity tensor is preserved. 

22. 2018, URLNet: Learning a URL Representation with Deep Learning for Malicious URL Detection, https://arxiv.org/pdf/1802.03162.pdf

23. ECCV 2020, SegFix: Model-Agnostic Boundary Refinement for Segmentation, https://arxiv.org/pdf/2007.04269.pdf , a model-agnostic post-processing scheme to improve the boundary quality for the segmentation result that is generated by any existing segmentation model. SegFix consistently reduces the boundary errors for segmentation results. The number of error pixels significantly decrease with larger distances to the boundary, predictions of the interior pixels are more reliable. Reduce boundary errors by replacing labels of boundary pixels with the labels of corresponding interior pixels for a segmentation result(The first step aims to localize the pixels along the object boundaries. We follow the contour detection methods and simply use a convolutional network to predict a binary mask indicating the boundary pixels. In the second step, we learn a direction away from the boundary pixel to an interior pixel and identify the corresponding interior pixel by moving from the boundary pixel along the direction by a certain distance). We only need to train a single unified SegFix model once without any further fine-tuning the different segmentation models(across multiple different datasets). Illustrating the SegFix framework: in the training stage, we first send the input image into a backbone to predict a feature map, then we apply a boundary branch to predict a binary boundary map and a direction branch to predict a direction map and mask it with the binary boundary map, we apply boundary loss and direction loss on the predicted boundary map and direction map separately, in the testing stage, we first convert the direction map to offset map and then refine the segmentation results of any existing methods according to the offset map. 

24. 2019, Devil is in the Edges: Learning Semantic Boundaries from Noisy Annotations, https://arxiv.org/pdf/1904.07934.pdf ,aim to learn sharp and precise semantic boundaries by explicitly reasoning about annotation noise during training. Semantically Thinned Edge Alignment Learning(STEAL) approach is agnostic to the backbone CNN architecture, and can be plugged on top of any existing learning-based boundary detection network. 

25. 2017, Improved Regularization of Convolutional Neural Networks with Cutout, https://arxiv.org/pdf/1708.04552.pdf ,cutout: regularization technique, randomly masking out square regions of input during training. Initially developed cutout as a targeted approach that specifically removed important visual features from the input of the image, this approach was similar to max-drop, in that we aimed to remove maximally activated features in order to encourage the network to consider less prominent features. Targeted coutout method performed well, but randomly removing regions of a fixed size performed just as well as the targeted approach, without requiring any manipulation of the feature maps. The size of the cutout region is an important hyper-parameter. 

26. 2018, Attention U-Net:Learning Where to Look for the Pancreas, https://arxiv.org/pdf/1804.03999.pdf
 Models trained with Attention Gate(AGs) implicitly learn to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task.
 Image-grid based gating that allows attention coefficients to be specific to local regions. This improves performance compared to gating based on a global feature vector.
 Schematic of the proposed additive attention gate(AG). Input features(x) are scaled with attention coefficients(a) computed in AG. Spatial regions are selected by analysing both the activations and contextual information provided by the gating signal(g) which is collected from a coarser scale. Grid resampling of attention coefficients is done using trilinear interpolation.

27. 2018, UNet++: A Nested U-Net Architecturefor Medical Image Segmentation, https://arxiv.org/pdf/1807.10165.pdf
 A deeply-supervised(deep supervision --> multi-segmentation loss) encoder-decoder network where the encoder and decoder sub-networks are connected through a series of nested, dense skip pathways(re-designed dense connection).
 Model pruning at the cost of accuracy degradation.

28. 2020, Image Segmentation Using Deep Learning:A Survey, https://arxiv.org/pdf/2001.05566.pdf
Models, Datasets, Evaluations

29. 2017, W-Net: A Deep Model for Fully Unsupervised Image Segmentation, https://arxiv.org/pdf/1711.08506.pdf
W-shaped network such that it reconstructs the original input images and also predicts a segmentation map without any labeling information.
Soft Normalized Cut Loss(to predict segmentation) + Reconstruction Loss + CRF + Hierarchical Segmentation

30. 2019, One Network To Segment Them All: A General, Lightweight System for Accurate 3D Medical Image Segmentation, https://arxiv.org/pdf/1911.01764.pdf
Multi Planar UNet

31. 2020, Magnetic Resonance Images Based Brain Tumor Segmentation- A critical survey, https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9143045&casa_token=tn4Pcch49YwAAAAA:nn3jQvr0P2o_euf-urcKyoThFUvBU2yf_uhuYTyk63Fy0FWQyDqjpHahN6stYQMiK8WbY8nCPUZ6&tag=1
Benign: low-grade glioma(LGG), Malignant: high-grade glioma(HGG)
Medical images security can be achieved with the help of watermarking techniques

32. 2020, Histological Image Classification using Deep Features and Transfer Learning, https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9108668
CNN models(SqueezeNet, MobileNet, RestNet, DenseNet, one of them) with transfer learning as feature extractor, and then feature selection(Infinite Latent Feature Selection(ILFS):A Probabilistic Latent Graph-Based Ranking Approach), finally SVM classifier

33. 2019, EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks, https://arxiv.org/pdf/1905.11946.pdf
systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance.
use neural architecture search(NAS) to design a new baseline network and scale it up to obtain a family of models, called EfficientNets.
proposed compound scaling method that uniformly scales all three dimensions with a fixed ratio.
the compound scaling method makes sense because if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image.
the effectiveness of model scaling heavily depends on the baseline network; use neural architecture search(NAS) to develop a new baseline network, and scale it up to obtain a family of models, called EfficientNets.
Empirically observe that diferent scaling dimensions are not independent. Intuitively, for higher resolution images, we should increase network depth, such that the larger receptive fields can help capture similar features that include more pixels in bigger images. Correspondingly, we should also increase network width when resolution is higher, in order to capture more fine-grained patterns with more pixels in high resolution images. These intuitions suggest that we need to coordinate and balance different scaling dimensions rather than conventional single-dimension scaling.

34. 2019 CVPR, DFANet: Deep Feature Aggregation for Real-Time Semantic Segmentation, https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_DFANet_Deep_Feature_Aggregation_for_Real-Time_Semantic_Segmentation_CVPR_2019_paper.pdf ,our proposed network starts from a single lightweight backbone and aggregates discriminative features through sub-network and sub-stage cascade respectively, drawbacks of some real-time semantic segmentation approaches(restrict input image size, pruning redundant channels —> lose the spatial details around boundaries and small objects. Shallow network weakens feature discriminative ability). Adopt a lightweight backbone model(rather than ResNet, Xception and DenseNet backbone) and investigate how to improve the segmentation performance with limited computation. The high-level context is lacking in incorporation with the former level features which also retain the spatial detail and semantic information in the network path, in order to enhance the model learning capacity and increase the receptive field simultaneously, feature reuse is an immediate thought. Deep Feature Aggregation Network(DFANet) contains three parts: the lightweight backbones, sub-network aggregation and sub-stage aggregation modules. Because depth-wise separable convolution is proved to be one of the most efficient operation in real-time inference, we modify the Xception network as the backbone structure. In pursuit of better accuracy, we append a fully-connected attention module in the tail of the backbone to reserve the maximum receptive field. Sub-network aggregation focuses on upsampling the high-level feature maps of the previous backbone to the input of the next backbone to refine the prediction result. From another perspective, sub-network aggregation can be seen as a coarse-to-fine process for pixel classification. Sub-stage aggregation assembles feature representation between corresponding stages through “coarse” part and “fine” part. It delivers the receptive field and and high dimension structure details by combining the layers with the same dimension. After these three modules, a slight decoder composed of convolution and bilinear upsampling operations is adopted to combine the outputs of each stage to generate the coarse-to-fine segmentation results. Deep feature aggregation to tackle real-time semantic segmentation on high resolution image.

35. IEEE 2020, Contextual Attention Refinement Network for Real-Time Semantic Segmentation, https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9040632 ,learns an attention vector to guide the fusion of low-level and high-level features for obtaining higher segmentation accuracy. Various strategies can be used to generate more accurate segmentation results, e.g. designing a dilated convolution, combining features at different levels, and employing a multi-scale context aggregation strategy. Achieving a balance between accuracy and speed is critical for the semantic segmentation task in many real-time applications. Features learned from deeper layers contain relatively more semantic information. 

36. 2020, RTSEG: REAL-TIME SEMANTIC SEGMENTATION COMPARATIVE STUDY, https://arxiv.org/pdf/1803.02758.pdf ,various encoder and decoder combination comparison, decoders: SkipNet, UNet, Dialation Frontend.

37. 2016, ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation , https://arxiv.org/pdf/1606.02147.pdf ,ENet(Efficient Neural Network), SegNet saves indices of elements chosen in max pooling layers. Early downsampling: early stages of the network, which are often the most expensive by far. 

38. 2016, SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation, https://arxiv.org/pdf/1511.00561.pdf ,the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling(improves boundary delineation). 

39. 2017, Pyramid Scene Parsing Network, https://arxiv.org/pdf/1612.01105.pdf ,incorporate suitable global features, the local and global clues together make the final prediction more reliable. Global average pooling is a good baseline model as the global contextual prior, which is a commonly used in image classification tasks. 

40. CVPR 2018, ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation, https://arxiv.org/abs/1803.06815 ,efficient in terms of computation, memory, and power. Under the same constraints on memory and computation, ESPNet outperforms all the current efficient CNN networks such as MobileNet, ShuffleNet, and ENet on both standard metrics and newly introduced performance metrics that measure efficient on edge devices. The point-wise convolutions help in reducing the computation, while the spatial pyramid of dilated convolutions re-samples the feature maps to learn the representations from large effective receptive field. Convolutional factorization decomposes the convolutional operation into multiple steps to reduce the computational complexity. ESPNet first learns the encoder and then attaches a light-weight decoder to produce the segmentation mask. The ASP(Atrous spatial pyramid) module involves branching with each branch learning kernel at a different receptive field(using dilated convolutions). Though ASP modules tend to perform well in segmentation tasks due to their high effective receptive fields, ASP modules have high memory requirements and learn many more parameters. 

41. 2017, ERFNet: Efficient Residual Factorized ConvNet for Real-time Semantic Segmentation, http://www.robesafe.es/personal/eduardo.romera/pdfs/Romera17tits.pdf ,the core of our architecture is a novel layer that uses residual connections and factorized convolutions in order to remain efficient while retaining remarkable accuracy.  Similar paper with same authors: 2017, Efficient ConvNet for Real-time Semantic Segmentation, http://www.robesafe.uah.es/personal/eduardo.romera/pdfs/Romera17iv.pdf

42. 2017 CVPR, RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation, https://arxiv.org/pdf/1611.06612.pdf ,RefineNet exploits various levels of detail at different stages of convolutions and fuses them to obtain a high-resolution prediction without the need to maintain large intermediate feature maps.High-level semantic features helps the category recognition of image regions, while low-level visual features help to generate sharp, detailed boundaries for high-resolution prediction. Chained residual pooling is able to capture background context from a large image region, it does so by effectively pooling features with multiple window sizes and fusing them together with residual connections and learnable weights.

43. 2018, Light-Weight RefineNet for Real-Time Semantic Segmentation, https://arxiv.org/pdf/1810.03272.pdf ,replace 3*3 convolution with 1*1 convolution: receptive size issue, the skip-design structure of RefineNet, where low-level features are being summed up with the high-level ones and keeping chained residual pooling(CRP) blocks that are responsible for gathering contextual information. The CRP block tends to enlarge empirical receptive field(ERF), while the summation with the lower layer features further produces significantly larger activation contours. According to the ablation experiments, CRP is the main driving force behind accurate segmentation and classification, while the RCU(Residual Conv Unit) block improves the results just marginally(So this RCU block can be removed). This paper rethink an existing semantic segmentation architecture into the one suitable for real-time performance, while keeping the performance levels mostly intact.

44. 2018, ICNet for Real-Time Semantic Segmentation on High-Resolution Images, https://arxiv.org/pdf/1704.08545.pdf ,based on PSPNet, image cascade network(ICNet) that incorporates multi-resolution branches under proper label guidance to address this challenge. Cascade feature fusion unit and cascade label guidance strategy are proposed to integrate medium and high resolution features, which refine the coarse semantic map gradually. Only the lowest-resolution input is fed into the heavy CNN with much reduced computation to get the coarse semantic prediction, the higher-resolution inputs are designed to recover and refine the prediction progressively regarding blurred boundaries and missing details. ICNet achieves high-efficiency inference with reasonable-quality segmentation results.

45. 2016, Speeding up Semantic Segmentation for Autonomous Driving(SQNet), https://openreview.net/pdf?id=S1uHiFyyg ,ELU has a mean activation of 0, thereby avoiding any bias shift effect: in ReLU networks, units will typically have a non-zero mean activation, thus they will act as additional bias unit for units in the next layer. By enabling units to have zero mean, this bias shift effect is reduced, which makes it easier for units to focus solely on actual information processing. This could otherwise only be achieved by using batch normalization, which would increase the computational cost of the network by adding specific layers to perform this operation. 

46. 2015, Fully Convolutional Networks for Semantic Segmentation(FCN), https://arxiv.org/pdf/1411.4038.pdf ,patch-wise training

47. 2018, BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation, https://arxiv.org/pdf/1808.00897.pdf ,design a spatial path with a small stride to preserve the spatial information and generate high-resolution features, a context path with a fast downsampling strategy is employed to obtain sufficient receptive field, a new feature fusion module to combine features efficiently. For spatial path, we stack only three convolution layers to obtain the 1/8 feature map, which retains affluent spatial details. In respect of context path, we append a global average pooling layer on the tail of Xception, where the receptive field is the maximum of the backbone network. The output feature of spatial path is low level, while the output feature of context path is high level, therefore, we propose a specific feature fusion module to fuse these features. 

48. 2019, On Boosting Semantic Street Scene Segmentationwith Weak Supervision, https://arxiv.org/pdf/1903.03462.pdf, image with strong(per-pixel) or weak(bounding boxes and image-level) labels. The performance of our method heavily depends on two factors: one for the amount of weak labels and their semantic extent of class connotation and another for the domain gap between strongly and weakly labeled datasets. Image level + boundary level + pixel level: image level for class attention, and then use boundary level for boundary attention, and then pixel level segmentation.

49. 2019, A DEEP GRADIENT BOOSTING NETWORK FOR OPTIC DISC AND CUP SEGMENTATION, https://arxiv.org/pdf/1911.01648.pdf ,To make use of spatial details and contain rich semantic information, making predictions on multiple levels of features, the final outputs are obtained by element-wise summation. In multiple prediction branches design, these branches are independent, which limits the learning capacity of the segmentation model. Our BoostNet is designed for pixel-level classification which treats the inner sub-networks as weaker learners while BoostCNN is proposed for image-level classification which treats the classification models after different training iterations or trained with different inputs as weaker learners.

50. (GCN)2017, SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS, https://arxiv.org/pdf/1609.02907.pdf

51. 2020, DoubleU-Net: A Deep Convolutional Neural Network for Medical Image Segmentation, https://ieeexplore.ieee.org/document/9183321 , https://arxiv.org/pdf/2006.04868.pdf ,GitHub: https://github.com/DebeshJha/2020-CBMS-DoubleU-Net

52. Top 10 BraTS2020 challenge solution: Brain tumor segmentationwith self-ensembled, deeply-supervised 3D-Unet like neural networks. https://arxiv.org/pdf/2011.01045.pdf ,The Hausdorff distance is complimentary to the Dice metric, as it measures the maximal distance between the margin of each set. It penalizes greatly outliers, as a prediction could exhibit almost voxel-perfect overlap, but if a single voxel is far away from the reference segmentation, the Hausdorff distance will be high. Group normalization and Instance normalization were used as a replacement for Batch Normalization due to a small batch size during training and good theoretical performance on non-medical datasets. Previous winner of the BraTS challenge limited their downsampling steps to 3, we hypothesized that further downsampling of the feature maps, given the limited size of the input(128*128*128), would lead to irreversible loss of spatial information. As the last stage of the encoder tasks much less GPU memory than the first, the dilation trick was used to perform a pseudo fifth stage at the same spatial resolution as the fourth stage. Deep supervision was performed after the dilated convolutions of the bottom of the network, and after each stage of the decoder(except the last), deep supervision was achieved by adding an extra 1*1*1 convolution with sigmoid activation and trilinear upsampling, the final loss is the unweighted sum of the main output and the three auxiliary losses. The saved weights were averaged, effectively creating a new "self-ensembled" model, we expected this method to produce a more generalizable model.

53. 2019, Two-Stage Cascaded U-Net: 1st Place Solution to BraTS Challenge 2019 Segmentation Task, https://www.researchgate.net/publication/341488149_Two-Stage_Cascaded_U-Net_1st_Place_Solution_to_BraTS_Challenge_2019_Segmentation_Task ,In the first stage, we use a variant of U-Net as the first stage network to train a coarse prediction, in the second stage, we increase the width of the network and use two decoders so as to boost performance. The coarse segmentation map is fed together with the raw images into the second stage U-Net. Due to GPU memory limitations, our networks is designed to take input patches of size 128*128*128 voxels and to use a batch size of one, the network architecture consists of a larger encoding path, to extract complex semantic features, and a smaller decoding path, to recover a segmentation map with the same input size. Only use a dropout with a rate of 0.2 after the initial encoder convolution. In the second stage, the structure of the two decoders is the same except that one uses a deconvolution and the other uses trilinear interpolation, the interpolation decoder is used only druing training, because the performance of the decoder used deconvolution is better than used trilinear interpolation and add a decoder used trilinear interpolation to regularize the shared encoder can improve the performance in our experiment. Soft Dice loss. Since the MRI intensity values are non-standardized, we apply intensity normalization to each MRI modality from each patient independently by subtracting the mean and dividing by the standard deviation of the brain region only. Gradient checkpointing by PyTorch to reduce the memory consumption. To obtain a more robust prediction, we preserve eight weights of the model in the last time of the training progress for prediction and finally we average the output of the resulting eight segmentation probability maps.


54. 2018, 3D MRI brain tumor segmentation usingautoencoder regularization, https://arxiv.org/pdf/1810.11654.pdf ,Due to a limited training dataset size, a variational auto-encoder branch is added to reconstruct the input image itself in order to regularize the shared decoder and impose additional constraints on its layers. We follow the encoder-decoder structure of CNN, with asymmetrically large encoder to extract deep image features, and the decoder part reconstructs dense segmentation masks, we also add the variational autoencoder(VAE) branch to the network to reconstruct the input images jointly with segmentation in order to regularize the shared encoder, at inference time, only the main segmentation encoder-decoder part is used. Group Normalization shows better than BatchNorm performance when batch size is small(batch size of 1 in this case). We use the spatial dropout with a rate of 0.2 after the initial encoder convolution, we have experimented with other placements of the dropout(including placing dropout layer after each convolution), but did not find any additional accuracy improvements. 

55. 2019, A large annotated medical image dataset for the development and evaluation of segmentation algorithms, https://arxiv.org/pdf/1902.09063.pdf ,Dataset Summation:(1. Brain Tumor: https://www.cancerimagingarchive.net/, 2. Many kind of medical image data: http://medicaldecathlon.com/ )

56. 2017, Ensembles of Multiple Models and Architecturesfor Robust Brain Tumour Segmentation, https://arxiv.org/pdf/1711.01468.pdf ,One route to address the bias/variance dilemma is ensembling. 

57. 2019, One Network To Segment Them All:A General, Lightweight System for Accurate 3DMedical Image Segmentation, https://arxiv.org/pdf/1911.01764.pdf ,The system relies on multi-planar data augmentation which facilitates the application of a single 2D structure based on the familiar U-Net. 

58. 2020, KiU-Net: Towards Accurate Segmentation ofBiomedical Images using Over-completeRepresentations, https://arxiv.org/pdf/2006.04878.pdf ,In studies, we observe that there is a considerable performance drop in the case of detecting smaller anatomical structures with blurred noisy boundaries. Address this issue by proposing an over-complete architecture(Ki-Net) which involves projecting the data onto higher dimensions(in the spatial sense). In the case of standard U-Net, even with skip connections, the smallest receptive field is limited by that of the first layer, hence, under-complete architectures are essentially limited in their abilities to capture finer details. In summary, this paper(1)explores over-complete deep networks(Ki-Net) for the task of segmentation(2)Propose a novel architecture(KiU-Net) combining the features of both under-complete and over-complete deep networks which captures finer details better than the standard encoder-decoder architecuture of U-Net thus aiding in precise segmentation, and(3)achieves faster convergence and better performance metrics than recent methods for segmentation. Propose a cross residual fusion block(CRFB), this block extracts complementary features from both network branches and forwards to both of them respectively. 

59. 2018, SEMI-SUPERVISED LEARNING FOR PELVIC MR IMAGE SEGMENTATION BASED ONMULTI-TASK RESIDUAL FULLY CONVOLUTIONAL NETWORKS, https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8363713&tag=1 ,Specifically, we first train an initial multi-task residual fully convolutional network(FCN) based on a limited number of labeled MRI data. Based on the initially trained FCN, those unlabeled new data can be automatically segmented and some reasonable segmentations(after manual/automatic checking) can be included into the training data to fine-tune the network, this step can be repeated to progressively improve the training of our network, until no reasonable segmentations of new data can be included. The results show that, using our proposed semi-supervised learning to enlarge the training data could better improve the segmentation performance, also, progressively including the unlabeled data into the training set for updating the network parameters can perform better than the conventional semi-supervised learning. 

60. 2018, Semi-supervised Skin Lesion Segmentationvia Transformation ConsistentSelf-ensembling Model, https://arxiv.org/pdf/1808.03887.pdf ,We observe that in the segmentation problem, if one transforms(e.g. rotate)the input image, the expected prediction should be transformed in the same manner. Actually, when the inputs of CNNs are rotated, the corresponding network predictions would not rotated in the same way. In this regard, we take advantages of this property by introducing a transformation(i.e.rotation,flipping) consistent scheme at the input and output space of our network. Specifically, we design the unsupervised/regularization loss by minimizing the differences between the network predictions under different transformations of the same input. To better utilize the unlabeled data for segmentation tasks, we propose a transformation consistent scheme in self-ensembling model and demonstrate the effectiveness for semi-supervised learning. The convolutional network consisting of a series of convolutions is also not transformation equivariant. We can regularize the network to be transformation consistent and further increase the network generalization capacity. Our semi-supervised method can improve the robustness of the network due to the regularization effect of the unsupervised loss.

61. 2018， ADVERSARIAL LEARNING FOR SEMI-SUPERVISED SEMANTIC SEGMENTATION， https://openreview.net/pdf?id=SJQO7UJCW ，Different from the typical generators that are trained to generate images given noise vectors, our segmentation network outputs the probability maps of the semantic labels given an input image. Under this setting, we wish to push the outputs of the segmentation network as close as the ground truth label maps spatially. We utilize the confidence maps generated by our discriminator network as the supervisory signal to guide the cross-entropy loss in a “self-taught” manner. The confidence maps indicate which regions of the prediction distribution are close to the ground truth label distribution, so that the segmentation network can trust these predictions and hence can be trained via a masked cross-entropy loss. By adopting the proposed framework, we show that the segmentation accuracy can be further improved by adding images without any annotations in the domain of labeled images. In this work, we propose an adversarial learning scheme for semi-supervised semantic segmentation. We train a fully convolutional discriminator network to enhance the segmentation network with both labeled and unlabeled data. With labeled data, the adversarial loss for the segmentation network is designed to learn higher order structural information without post-processing. For unlabeled data, the confidence maps generated by the discriminator network act as the self-taught signal for refining the segmentation network. 

62. 2019, Prediction of drug-target interaction by integrating diverse heterogeneous information source with multiple kernel learning and clustering methods, https://www.sciencedirect.com/science/article/pii/S1476927118307771?via%3Dihub ,based on the assumption that similar drugs tend to interact with similar proteins and vice versa. 

63. 2017, Semi-Supervised Learning for Network-BasedCardiac MR Image Segmentation, https://spiral.imperial.ac.uk/bitstream/10044/1/49165/2/bai2017miccai.pdf ,:(similar idea of mine:)

64. 2017, Semi-Supervised Deep Learning for Fully Convolutional Networks, https://arxiv.org/pdf/1703.06000.pdf ,This embedding loss function aims at minimizing the distance among latent representations of similar h(x_i) and h(x_j) of neighbouring data samples x_i and x_j. 

65. 2016, Long-term Recurrent Convolutional Networks for Visual Recognition and Description, https://arxiv.org/pdf/1411.4389.pdf ,We’ve presented LRCN, a class of models that is both spatially and temporally deep, and flexible enough to be applied to a variety of vision tasks involving sequential inputs and outputs.

66. 2020, DPDDI: a deep predictor for drug-drug interactions, https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-03724-x ,extract feature vectors via GCN and then feed them features into DNN to predict interactions.

67. 2019, Semi-Supervised Medical Image Segmentation via Learning Consistency under Transformations, https://arxiv.org/pdf/1911.01218.pdf ,We propose a novel semi-supervised method that, in addition to supervised learning on labeled training images, learns to predict segmentations consistent under a given class of transformations on both labeled and unlabeled images. More specifically, in this work we explore learning equivariance to elastic deformations. We implement this through:1)a siamese architecture with two identical branches, each of which receives a differently transformed image, and 2)a composite loss function with a supervised segmentation loss term and an unsupervised term that encourages segmentation consistency between the predictions of the two branches. We implement this consistency learning through a Siamese architecture trained end-to-end. The network has two identical branches each of which receives differently transformed versions of the same images as inputs and is supervised to output segmentations consistent with the other branch, in addition to learning from labeled images in a supervised fashion. Proposed Siamese approach does not train the network to fit specific targets on unlabeled images(i.e. the synthetic labels), which are unknown and cannot be reliably estimated; it only encourages the outputs to have the desired transformation consistency property. 

68. 2021, Refining activation downsampling with SoftPool, https://arxiv.org/pdf/2101.00440.pdf A novel pooling method that better preserves informative features and, consequently, improves classification performance in CNNs. SoftPool uses the softmax of inputs within a kernel region where each of the activations has a proportional effect on the input.

69. 2021, Generative Adversarial U-Net for Domain-free Medical Image Augmentation, https://arxiv.org/pdf/2101.04793.pdf Use conditional GANs, which learn a mapping from a random noise vector and observed images for the class. Maybe can be used in semi-supervised segmentation.

70. 2020, Semi-supervised Cardiac Image Segmentation via Label Propagation and Style Transfer, https://arxiv.org/pdf/2012.14785.pdf , A 3D UNet is trained on the data with both manual and pseudo labels. At last, as those pseudo labels is not as accurate as manual ones, the trained UNet is fine-tuned on the manually labelled data, Furthermore, in order to reduce the gap of the data from different vendors, we augment the training data through style transfer to improve the generalization of semi-supervised learning. Data from different time frames within a patient share almost identical distribution, which will alleviate label propagation errors caused by inter-subject variabilities.

Dataset, MICCAI, semi-supervised learning dataset(labeled with unlabeled data), Multi-Centre, Multi-Vendor & Multi-Disease Cardiac Image Segmentation Challenge (M&Ms), https://www.ub.edu/mnms/ The M&Ms challenge is motivated to contribute to the effort of building generalizable models that can be applied consistently across clinical centers. In this challenge, the cohort is composed of 350 patients with hypertrophic and dilated cardiomyopathies as well as healthy subjects. All subjects were scanned in clinical centers in three different countries(Spain, Germany and Canada) using four different magnetic resonance scanner vendors(Siemens, General Electric, Philips and Canon). In M&Ms challenge, only the End of Systole(ES and the End of Diastole(ED) of the cardiac MRI sequence timeframes are annotated. 

71. 2021, Sequential semi-supervised segmentation for serial electron microscopy image with small number of labels, https://www.sciencedirect.com/science/article/pii/S0165027021000017 ,worked on serial image segmentation in the framework of transductive learning. Sequential semi-supervised segmentation performs segmentation from a few labeled data by repeating pseudo-labeling. Assume a transductive setting that predicts all labels in a dataset from only partially obtained labels while avoiding the pursuit of generalization performance for unknown data. Related work: self-training uses a model trained on labeled data to make inferences on unlabeled data. Then, among the prediction results, the predicted map with high confidence is regarded as a pseudo-label, and the model is retrained using it. By repeating this procedure, the performance of the model is improved. Github: https://github.com/eichitakaya/Sequential-Semi-supervised-Segmentation

72. 2020, Unlabeled Data Guided Semi-supervised Histopathology Image Segmentation, https://arxiv.org/pdf/2012.09373.pdf ,Transductive learning means in the training process, the test images are shown as unlabeled images to the model and in biomedical imaging, transductive learning has found wide applications. 

73. 2020, Semi-supervised Segmentation via Uncertainty Rectified Pyramid Consistency and Its Application to Gross Target Volume of Nasopharyngeal Carcinoma, https://arxiv.org/pdf/2012.07042.pdf ,Github: https://github.com/HiLab-git/SSL4MIS  Related work(good summary for semi-supervised learning): an iterative framework where in each iteration, pseudo labels for unannotated images are predicted by the network and refined by a Conditional random Field(CRF), then the new pseudo labels are used to update the network.(Similar idea of M) Method: A standard supervised loss at multiple scales is used for learning from labeled images and for unlabeled images, we encourage the multi-scale predictions to be consistent, which serves a regularization. 

74. 2019, Semi-Supervised Brain Lesion Segmentation with an Adapted Mean Teacher Model, https://arxiv.org/pdf/1903.01248.pdf ,assume that the segmentation should be consistent for similar input data, and define a segmentation consistency loss, which is computed for a pair of inputs that are obtained by adding noises to the same unannotated sample. In this way, unannotated data can be incorporated into the learning process and provide regularization information. The student model attempts to learn the targets generated by the teacher model. The teacher model may be generate reasonable target labels at the beginning, so adaptive weighting coefficient. The final teacher model is used to perform lesion segmentation for test samples. 

75. 2020, Transformation-consistent Self-ensembling Model for Semi-supervised Medical Image Segmentation, https://arxiv.org/pdf/1903.00348.pdf ,methods encourages consistent predictions of the network-in-training for the same input under different perturbations. The teacher and student models share the same architecture, and the weight of the teacher model is the exponential moving average(EMA) of the student model. Smoothness assumption: data points close to each other in the image space are likely to be close in the label space. Transformation invariant. Learning to minimize the output differences caused by these transformations will regularize the network to be transformation-consistent. 

76. 2016, Built-in Foreground/Background Prior for Weakly-Supervised Semantic Segmentation, https://arxiv.org/pdf/1609.00446.pdf ,VGG-16 backbone, weights trained on ImageNet for the task of object recognition. Weakly-supervised semantic segmentation that leverages foreground/background masks directly extracted from our network pre-trained for the tasks of object recognition. Another kind of weak supervision, consisting of selecting one mask among a set of candidates, and this procedure can be achieve very easily, taking only roughly 2-3 seconds per image, and yields a further significant boost in accuracy.

77. 2017, Exploiting Saliency for Object Segmentation from Image Level Labels, https://arxiv.org/pdf/1701.08261.pdf ,prior knowledge: objects of interest are rarely single pixel. In this work, author propose to exploit class-agnostic saliency as a new ingredient to train for class-specific pixel labelling. Object seeds: find high confidence points over the objects classes of interest. Fuse seeds(bakprop) and saliency(class-agnostic, trained from other datasets).  Generate psedo-labels and then train neural network models. 

78. 2019 ICCV, CAMEL: A Weakly Supervised Learning Framework for Histopathology Image Segmentation, https://arxiv.org/pdf/1908.10555.pdf ,CAMEL, a weakly supervised learning framework for histopathology image segmentation using only image-level labels. Using multiple instance learning (MIL)-based label enrichment, CAMEL splits the image into latticed instances and automatically generates instance-level labels. After label enrichment, the instance-level labels are further assigned to the corresponding pixels, producing the approximate pixel-level labels and making fully supervised training of segmentation models possible. One way to improve the performance of weakly supervised learning algorithms is to add more supervision constraints. This paper: automatically enriching labeling information instead of introducing artificial constraints before building the segmentation model is crucial for weakly supervised learning. This work achieves comparable segmentation results with its fully supervised counterparts. 
