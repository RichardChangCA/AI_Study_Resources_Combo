1. Medium, Reasons to Choose PyTorch for Deep Learning: https://towardsdatascience.com/reasons-to-choose-pytorch-for-deep-learning-c087e031eaca Deep learning researchers should study both PyTorch & TensorFlow 2.x but focus on only one of them.

2. Medium, Google Just Introduced TensorFlow Developer Certificate Exam: https://towardsdatascience.com/google-just-introduced-tensorflow-developer-certificate-exam-e15c385685b8 ,Tensorflow certificate granted by Google, tensorflow is wildly used in industries, as well as both pytorch and tensorflow are wildly used in academic fields.

3. Medium, 5 Important Changes Coming with TensorFlow 2.0： https://levelup.gitconnected.com/5-important-changes-coming-with-tensorflow-2-0-e6bb172c5fdf

- start to be more similar with numpy

- with keras

- clean up redundant APIs

- tf.datasets

- can run tf1.x in the env of tf.2.x , there is a srcipt to auto-transfer tf1.x to tf2.x

4. Medium, Visualising Filters and Feature Maps for Deep Learning: https://towardsdatascience.com/visualising-filters-and-feature-maps-for-deep-learning-d814e13bd671

5. Medium, Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization: https://www.coursera.org/lecture/deep-neural-network/exponentially-weighted-averages-duStO ，some optimizers are illustrated well, like adam, RMSprop, learning rate decay, momentum, etc.

6. Medium, Deep Learning using GPU on your MacBook https://towardsdatascience.com/deep-learning-using-gpu-on-your-macbook-c9becba7c43 ,use the GPU on your macbook pro to speed up your deep learning models with keras

7. Medium, Deep Learning Course notebooks worth $2,000 are now open source: https://towardsdatascience.com/deep-learning-course-notebooks-worth-2-000-are-now-open-source-7d6bc759ef47 ,GitHub open source book repo: https://github.com/fastai/fastbook , online courses video: https://course.fast.ai/videos/?lesson=1 , the open source book: Deep Learning for Coders with fastai & PyTorch

8. Youtube, Why Does Batch Normormalization Work? (C2W3L06): https://www.youtube.com/watch?v=nUUqwaxLnWs , batch normalization can speed up the training process because the covariance shift can shift the mini-batch data distribution into normal distribution(which can represent the whole dataset distribution well), and have "slight" regularization effect because normalization is processed within mini-batch(maybe not representative), which generates a little noise. 

9. Medium, Probably the Best Resource to Learn Deep Learning in 2020: https://towardsdatascience.com/probably-the-best-resource-to-learn-deep-learning-in-2020-66d13a8ab1f1 ,code out neurons, layers, activation functions, optimizers, gradients, backpropagation — pretty much everything — completely from scratch, for understanding deep learning deeper. (but no cnn and rnn includes)
